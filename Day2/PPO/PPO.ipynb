{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b3562ed",
   "metadata": {},
   "source": [
    "# PPO\n",
    "\n",
    "Proximal Policy Optimization\n",
    "\n",
    "### Policy gradient methods\n",
    "\n",
    "Policy gradient methods are group of RL methods that attempt to find a policy gradient, then using a gradient ascent algorithm. Generally, this involves alternating between sampling and optimization. Many samples are taken to construct a policy gradient, this policy gets optimized, then more samples are taken. In reality, there exists a large variety of policy gradient methods, each seeking to do the same thing with a distinct strategy: Utilize gradient ascent on the policy gradient.\n",
    "\n",
    "### Actor-critic structure\n",
    "\n",
    "PPO uses an actor-critic structue. This means that it uses a dynamic involving two neural networks, were one network is an actor network (also known as the policy network), that converts the observations into actions, while the other network is the critic network (also known as the value network), which evaluates the performance of the actor network by recieving the observations as a input and outputing a discounted return. In this way, both the actor and the critic are learning during training, allowing for more complex value estimations.  \n",
    "\n",
    "While actually running the agent, only the actor network is used. The critic network only exists for use during the training process.\n",
    "\n",
    "### How does PPO do this?\n",
    "\n",
    "From Proximal Policy Optimization paper by OpenAI: https://doi.org/10.48550/arXiv.1707.06347\n",
    "\n",
    "![Alt text](PPODescription.png)\n",
    "\n",
    "$$r_{t}(\\theta)=/frac{\\pi_{\\theta}(a_{t}|s_{t})}{\\pi_{\\theta_{old}}(a_{t}|s_{t})}$$\n",
    "\n",
    "### Why PPO?\n",
    "\n",
    "PPO has been shown to excel at continous environments, which happen to be the environments that we face in robotics. It strikes a great balance between reliability and sample efficiency, outpreforming Deep Q-Networks in a variety of scenarios. Because of this, PPO is available in many of the frameworks included in IsaacLab, which we will be using next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194d4323",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b51d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "%pip install torchrl\n",
    "%pip install gym[mujoco]\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b45ab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from torch import multiprocessing\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch import nn\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.envs import (Compose, DoubleToFloat, ObservationNorm, StepCounter,\n",
    "                          TransformedEnv)\n",
    "from torchrl.envs.libs.gym import GymEnv\n",
    "from torchrl.envs.utils import check_env_specs, ExplorationType, set_exploration_type\n",
    "from torchrl.modules import ProbabilisticActor, TanhNormal, ValueOperator\n",
    "from torchrl.objectives import ClipPPOLoss\n",
    "from torchrl.objectives.value import GAE\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c93aabb",
   "metadata": {},
   "source": [
    "#### Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6352967",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "print(device)\n",
    "num_cells = 128  # number of cells in each layer i.e. output dim.\n",
    "lr = 3e-4\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Data collection parameters\n",
    "\n",
    "frames_per_batch = 1000\n",
    "total_frames = 50_000\n",
    "\n",
    "# PPO parameters\n",
    "sub_batch_size = 64  # cardinality of the sub-samples gathered from the current data in the inner loop\n",
    "num_epochs = 10  # optimization steps per batch of data collected\n",
    "clip_epsilon = (\n",
    "    0.2  # clip value for PPO loss: see the equation in the intro for more context.\n",
    ")\n",
    "gamma = 0.99\n",
    "lmbda = 0.95\n",
    "entropy_eps = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d750a7dd",
   "metadata": {},
   "source": [
    "#### Define environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a9f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_env = GymEnv(\"InvertedPendulum-v5\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0506fd31",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a44397",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    base_env,\n",
    "    Compose(\n",
    "        # normalize observations, making observations somewhat fit a gaussian distribution is preferable\n",
    "        ObservationNorm(in_keys=[\"observation\"]),\n",
    "        DoubleToFloat(),\n",
    "        StepCounter(),\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Set the parameters for the ObservationNorm() transform. Defining how the observation is normalized\n",
    "env.transform[0].init_stats(num_iter=1000, reduce_dim=0, cat_dim=0)\n",
    "print(\"normalization constant shape:\", env.transform[0].loc.shape)\n",
    "\n",
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb385cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"observation_spec:\", env.observation_spec)\n",
    "print(\"reward_spec:\", env.reward_spec)\n",
    "print(\"input_spec:\", env.input_spec)\n",
    "print(\"action_spec (as defined by input_spec):\", env.action_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee84f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = env.rollout(3)\n",
    "print(\"rollout of three steps:\", rollout)\n",
    "print(\"Shape of the rollout TensorDict:\", rollout.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f347b5f",
   "metadata": {},
   "source": [
    "#### Defining the policy network\n",
    "\n",
    "The policy network is what is considered as the actor in the PPO algorithm. Here, the neural network produces two outputs of the size of the action space. This includes a location output and a scale output. The scale output is fixed to be positive. To fit both of these outputs in the neural network, the last transformation maps to a size of 2 times the action space size. The other transformation are simply the number of cells, or neurons in the neural network, which were defined earlier,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948e5027",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = nn.Sequential(\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(2 * env.action_spec.shape[-1], device=device),\n",
    "    NormalParamExtractor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536f656c",
   "metadata": {},
   "source": [
    "#### Policy module\n",
    "\n",
    "The policy module is necessary to define the inputs and outputs of the neural network. This is where we write the two distinct outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd7de82",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_module = TensorDictModule(\n",
    "    actor_net, in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7898cac",
   "metadata": {},
   "source": [
    "Then, the policy module is used to build a distribution out of the location and scale of the previous normal distribution. This uses a probablistic actor class. Minimum and maximum values are also set, which come from the environment's specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990fe7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_module = ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=env.action_spec,\n",
    "    in_keys=[\"loc\", \"scale\"],\n",
    "    distribution_class=TanhNormal,\n",
    "       distribution_kwargs={\n",
    "        \"low\": env.action_spec.space.low,\n",
    "        \"high\": env.action_spec.space.high,\n",
    "    },\n",
    "    return_log_prob=True,\n",
    "    # we'll need the log-prob for the numerator of the importance weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88738192",
   "metadata": {},
   "source": [
    "### Value network\n",
    "\n",
    "The next important component of PPO is the value network. The value network is used to evaluate the actions. It is the \"critic\" in our actor-critic algorithm. It will take the observations as input and output a scalar value representing the expected return from that state. In this case the value network uses the same structure as the actor network, but with different parameters and weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4d1616",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_net = nn.Sequential(\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(1, device=device),\n",
    ")\n",
    "\n",
    "value_module = ValueOperator(\n",
    "    module=value_net,\n",
    "    in_keys=[\"observation\"],\n",
    ")\n",
    "\n",
    "print(\"Running policy:\", policy_module(env.reset()))\n",
    "print(\"Running value:\", value_module(env.reset()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777fc1a9",
   "metadata": {},
   "source": [
    "Replay buffer for stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ddbd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(max_size=frames_per_batch),\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f7346e",
   "metadata": {},
   "source": [
    "We will then use TorchRL's data collector class, which does three operations:\n",
    "\n",
    "1. reset the environment\n",
    "2. execute a step in the environment\n",
    "3. repeat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1010441",
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy_module,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    "    split_trajs=False,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcaebd4",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "For PPO to function, there needs to exist an advantage estimation, which balances bias and variance. Here, we will use TorchRL's GAE (general advantage estimator) module, which uses the value network defined previously.\n",
    "\n",
    "Next, TorchRL's ClipPPOLoss is used to limit going too far from the current policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2084ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "advantage_module = GAE(\n",
    "    gamma=gamma, lmbda=lmbda, value_network=value_module, average_gae=True, device=device,\n",
    ")\n",
    "\n",
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=policy_module,\n",
    "    critic_network=value_module,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_bonus=bool(entropy_eps),\n",
    "    entropy_coef=entropy_eps,\n",
    "    # these keys match by default but we set this for completeness\n",
    "    critic_coef=1.0,\n",
    "    loss_critic_type=\"smooth_l1\",\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optim, total_frames // frames_per_batch, 0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26944a89",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "1. Collect data\n",
    "   - Compute advantage\n",
    "     1. Loop \n",
    "     2. Back propagate\n",
    "     3. optimize\n",
    "     4. Repeat\n",
    "   - Repeat\n",
    "2. Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d05b6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = defaultdict(list)\n",
    "pbar = tqdm(total=total_frames)\n",
    "eval_str = \"\"\n",
    "\n",
    "# We iterate over the collector until it reaches the total number of frames it was\n",
    "# designed to collect:\n",
    "for i, tensordict_data in enumerate(collector):\n",
    "    # we now have a batch of data to work with. Let's learn something from it.\n",
    "    for _ in range(num_epochs):\n",
    "        # We'll need an \"advantage\" signal to make PPO work.\n",
    "        # We re-compute it at each epoch as its value depends on the value\n",
    "        # network which is updated in the inner loop.\n",
    "        advantage_module(tensordict_data)\n",
    "        data_view = tensordict_data.reshape(-1)\n",
    "        replay_buffer.extend(data_view.cpu())\n",
    "        for _ in range(frames_per_batch // sub_batch_size):\n",
    "            subdata = replay_buffer.sample(sub_batch_size)\n",
    "            loss_vals = loss_module(subdata.to(device))\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "\n",
    "            # Optimization: backward, grad clipping and optimization step\n",
    "            loss_value.backward()\n",
    "            # this is not strictly mandatory but it's good practice to keep\n",
    "            # your gradient norm bounded\n",
    "            torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    logs[\"reward\"].append(tensordict_data[\"next\", \"reward\"].mean().item())\n",
    "    pbar.update(tensordict_data.numel())\n",
    "    cum_reward_str = (\n",
    "        f\"average reward={logs['reward'][-1]: 4.4f} (init={logs['reward'][0]: 4.4f})\"\n",
    "    )\n",
    "    logs[\"step_count\"].append(tensordict_data[\"step_count\"].max().item())\n",
    "    stepcount_str = f\"step count (max): {logs['step_count'][-1]}\"\n",
    "    logs[\"lr\"].append(optim.param_groups[0][\"lr\"])\n",
    "    lr_str = f\"lr policy: {logs['lr'][-1]: 4.4f}\"\n",
    "    if i % 10 == 0:\n",
    "        # We evaluate the policy once every 10 batches of data.\n",
    "        # Evaluation is rather simple: execute the policy without exploration\n",
    "        # (take the expected value of the action distribution) for a given\n",
    "        # number of steps (1000, which is our ``env`` horizon).\n",
    "        # The ``rollout`` method of the ``env`` can take a policy as argument:\n",
    "        # it will then execute this policy at each step.\n",
    "        with set_exploration_type(ExplorationType.DETERMINISTIC), torch.no_grad():\n",
    "            # execute a rollout with the trained policy\n",
    "            eval_rollout = env.rollout(1000, policy_module)\n",
    "            logs[\"eval reward\"].append(eval_rollout[\"next\", \"reward\"].mean().item())\n",
    "            logs[\"eval reward (sum)\"].append(\n",
    "                eval_rollout[\"next\", \"reward\"].sum().item()\n",
    "            )\n",
    "            logs[\"eval step_count\"].append(eval_rollout[\"step_count\"].max().item())\n",
    "            eval_str = (\n",
    "                f\"eval cumulative reward: {logs['eval reward (sum)'][-1]: 4.4f} \"\n",
    "                f\"(init: {logs['eval reward (sum)'][0]: 4.4f}), \"\n",
    "                f\"eval step-count: {logs['eval step_count'][-1]}\"\n",
    "            )\n",
    "            del eval_rollout\n",
    "    pbar.set_description(\", \".join([eval_str, cum_reward_str, stepcount_str, lr_str]))\n",
    "\n",
    "    # We're also using a learning rate scheduler. Like the gradient clipping,\n",
    "    # this is a nice-to-have but nothing necessary for PPO to work.\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43801e09",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6606f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(logs[\"reward\"])\n",
    "plt.title(\"training rewards (average)\")\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(logs[\"step_count\"])\n",
    "plt.title(\"Max step count (training)\")\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(logs[\"eval reward (sum)\"])\n",
    "plt.title(\"Return (test)\")\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(logs[\"eval step_count\"])\n",
    "plt.title(\"Max step count (test)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
