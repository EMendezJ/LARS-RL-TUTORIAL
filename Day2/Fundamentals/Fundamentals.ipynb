{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faa97e6f",
   "metadata": {},
   "source": [
    "# Overview of RL fundamentals\n",
    "\n",
    "## The Q-learning strategy\n",
    "\n",
    "Our goal is to maximize the discounted, cumulative reward (the return). \n",
    "\n",
    "In theory, to maximize return, the correct action must be taken for every different state. To determine which is the correct action, the model needs to estimate the expected return for each action given a state, which is a tricky problem. This value we are trying to estimate is called Q-value, or action value. If we know the action value of every posible action at every possible state, choosing the action with the highest value at every state should give the maximum return (the problem is solved).\n",
    "\n",
    "#### Temporal difference and discount factor\n",
    "\n",
    "To actually calculate an action's value, it needs to be compared with the next state-action pair's value. This difference is known as the temporal difference. The different ways in which the next action's value is calculated distinguish different reinforcement learning algorithms. In this case, we use the best possible action in the next state as reference, to try to approximate the best Q function directly. This is based on the Bellman equation. In general, this strategy results in a more efficient algorithm. \n",
    "\n",
    "In the end, with Q-learning, the experienced value for an action is the sum of the direct reward for that action and the best possible value for the next state-action pair. To balance the value attributed in the immediate reward with the value attributed to future action values, we use a discount factor $\\gamma$. This value ranges from 0 to 1, determining how far-sighted the agent is. The experienced action values are calculated as follows: \n",
    "\n",
    "Experienced value $= r_{t} + \\gamma * Q(s_{t+1}, a_{best})$\n",
    "\n",
    "#### Learning rate\n",
    "\n",
    "When a model performs an action and recieves feedback, it must incorporate this feedback to \"learn\" from this experience. This means that the known action values must be updated to match this experience. However, simply setting the known value for that action to the experienced value in this instance would create an unstable model, as unknown probabilities can influence the result. Insted, the value that resulted from the action taken is compared to the currently known action value, and the known value for that action is moved towards the experienced value by a learning rate (lr). Higher learning rates should mean faster convergence, but can also cause imprecision and instability.\n",
    "\n",
    "#### Exploration vs exploitation\n",
    "\n",
    "For an RL model to learn, it must experience the environment. So, training involves realizing the actions and receiving feedback from them. While the model is training, it selects its actions based on its state, and the currently known action values of the actions in that state. So, as previously mentioned, choosing the action with the highest action value should bring the best result. However, the action values are always just estimates, which means there is a chance that an unkown set of actions can lead to an unexpectedly high return. So, to learn the true best action values, it is necessary to explore alternatives instead of sticking to the currently known best action. \n",
    "\n",
    "So, when selecting an action, there is a probability that the action is selected randomly instead of based on action values. This probability is called epsilon. In theory, epsilon should be high when training begins, but should be gradually lowered as the model learns more about the environment, and the known action values are more likely to be accurate. Choosing a random action is known as exploration, while choosing the known best action is known as exploitation. Too much exploitation and the model will not learn enough about the environment, while too much exploration can cause the model to waste time and not converge in a reasonable amount of time.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "1. The agent begins picking actions at random, assuming equal value to every action.\n",
    "2. The model estimates an action's value through both its reward and the value of the best known next action.\n",
    "3. The corresponding action's value gets updated to reflect this estimated value according to a learning rate.\n",
    "4. The agent now sometimes takes the best possible action and sometimes explores, according to the exploration factor epsilon.\n",
    "5. Over time, the difference between an action's known value and its estimated value decreases, as does epsilon, resulting in convergence.\n",
    "\n",
    "\n",
    "### Implementation of neural networks to form deep Q networks\n",
    "\n",
    "When neural networks are involved, the logic seemingly falls apart\n",
    "\n",
    "To see how this works, this simple cartpole example from pytorch is used,\n",
    "\n",
    "### Required dependencies for RL example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96361cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# For general use\n",
    "import math\n",
    "import random\n",
    "%pip install matplotlib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "# For neural networks\n",
    "%pip install torch\n",
    "%pip install numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# For environment generation\n",
    "%pip install gymnasium\n",
    "%pip install pygame\n",
    "import gymnasium as gym\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d38598",
   "metadata": {},
   "source": [
    "#### Select environment and set up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a243504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select cartpole environment\n",
    "env = gym.make(\"CartPole-v1\",render_mode=\"human\")\n",
    "\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# Check if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "print(device)\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5942f966",
   "metadata": {},
   "source": [
    "#### Setup transition storage for the agent's memory\n",
    "\n",
    "This memory can be sampled randomly, which should stabilize training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0159da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146b633f",
   "metadata": {},
   "source": [
    "### Creating neural network class for deep Q-learning\n",
    "This is based on pytorch's generic neural network module, which uses some key functions:\n",
    "\n",
    "- nn.Linear(): applies a linear transform using stored weights and biases\n",
    "- F.relu(): applies a non-linear transform, allowing the neural network to predict non-linear behaviours (Note: other non-linear transforms can be used)\n",
    "- self.layer: the layer of the neural network, essentially the steps that it follow to transform the inputs into outputs\n",
    "\n",
    "Neural networks are highly flexible, and should be studied on their own, but they are not the topic of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc34745",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc19513",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "\n",
    "- Batch size: Number of transitions \"in memory\" (transitions in replay buffer).\n",
    "- Gamma: Discount factor.\n",
    "- Epsilon: The rate of exploration. Starts with a high value and decreases exponentially to help the model converge.\n",
    "- LR: Learning rate.\n",
    "- Tau: Update rate for target network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d41912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 2500\n",
    "TAU = 0.005\n",
    "LR = 3e-4\n",
    "\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8780ee",
   "metadata": {},
   "source": [
    "### Action selection using epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fdb8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3d10b3",
   "metadata": {},
   "source": [
    "#### Plotting progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937a218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dc1534",
   "metadata": {},
   "source": [
    "### Model improvement step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb370da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8717e240",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = torch.Tensor([1])\n",
    "value.view(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e870b0b7",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eda35a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 150\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
